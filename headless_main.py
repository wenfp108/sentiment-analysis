import pandas as pd
import json
import logging
import os
import requests
from datetime import datetime

# å¼•å…¥é¡¹ç›®ç°æœ‰æ¨¡å—
from src.pipelines import top_posts_subreddit_pipeline
from src.logger_config import setup_logger
from src.eda import get_top_n_words

logger = setup_logger()

# === å½±å­æŒ‡æŒ¥ä¸­å¿ƒé…ç½® ===
COMMAND_REPO = "wenfp108/Central-Bank"  # ä½ çš„ç§äººæŒ‡ä»¤åº“
OUTPUT_FILE = "sentiment_report.json"
POST_LIMIT = 15      # æ¯ä¸ªæ¿å—æŠ“å–è´´æ•°
COMMENT_LIMIT = 30   # æ¯ä¸ªå¸–å­è¯„è®ºåˆ†ææ•°

def fetch_missions_from_shadow_hq():
    """
    å» GitHub Issue æ‹¿ä»»åŠ¡
    """
    # ä¼˜å…ˆä½¿ç”¨ GH_PAT (åœ¨ Secrets é‡Œé…ç½®)ï¼Œå…¶æ¬¡ GITHUB_TOKEN
    token = os.environ.get("GITHUB_TOKEN")
    
    if not token:
        logger.error("âŒ ç¼ºå°‘ GITHUB_TOKENï¼Œæ— æ³•è¿æ¥æŒ‡æŒ¥ä¸­å¿ƒï¼")
        return {}

    url = f"https://api.github.com/repos/{COMMAND_REPO}/issues?state=open&per_page=100"
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/vnd.github.v3+json"
    }

    logger.info(f"ğŸ“¡ æ­£åœ¨è¿æ¥: {COMMAND_REPO} ...")
    
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        resp.raise_for_status()
        issues = resp.json()
        
        missions = {}
        
        for issue in issues:
            title = issue.get('title', '').strip()
            
            # ğŸ¯ è¯†åˆ« [reddit]
            if '[reddit]' in title.lower():
                # 1. æå–æ¿å—å (å»æ‰æ ‡ç­¾)
                subreddit = title.lower().replace('[reddit]', '').strip()
                
                # 2. æå–å…³é”®è¯ (ç›´æ¥è¯»æ­£æ–‡ï¼Œç”¨é€—å·åˆ†éš”)
                body = issue.get('body', '')
                if body:
                    keywords = [k.strip() for k in body.replace('ï¼Œ', ',').replace('\n', ',').split(',') if k.strip()]
                else:
                    keywords = [] # æ­£æ–‡æ²¡å†™å°±ä¸ç­›å…³é”®è¯
                
                missions[subreddit] = keywords
                logger.info(f"ğŸ“¥ é¢†å–ä»»åŠ¡: r/{subreddit} | å…³é”®è¯: {keywords if keywords else 'æ—  (å…¨é‡æ¨¡å¼)'}")

        logger.info(f"âœ… è·å–åˆ° {len(missions)} ä¸ªæ¿å—ä»»åŠ¡ã€‚")
        return missions

    except Exception as e:
        logger.error(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return {}

def calculate_distribution(df):
    """è®¡ç®—æƒ…ç»ªåˆ†å¸ƒ"""
    if df.empty:
        return {"POSITIVE": "0%", "NEGATIVE": "0%", "NEUTRAL": "0%"}
    col_name = 'sentiment_clean_title_label'
    if col_name not in df.columns:
        return {"Error": "Column not found"}
    counts = df[col_name].value_counts(normalize=True)
    return {
        "POSITIVE": f"{counts.get('pos', 0):.0%}",
        "NEGATIVE": f"{counts.get('neg', 0):.0%}",
        "NEUTRAL":  f"{counts.get('neu', 0):.0%}"
    }

def run_mission():
    final_report = []
    
    # 1. é¢†ä»»åŠ¡
    monitor_matrix = fetch_missions_from_shadow_hq()
    
    if not monitor_matrix:
        logger.warning("ğŸš« æŒ‡æŒ¥ä¸­å¿ƒæ²¡æœ‰ [reddit] ä»»åŠ¡ã€‚")
        return

    logger.info("ğŸš€ Woonbot å¯åŠ¨...")

    # 2. è·‘ä»»åŠ¡
    for subreddit, keywords in monitor_matrix.items():
        logger.info(f"ğŸ“¡ æ­£åœ¨æŠ“å– r/{subreddit} (Hotæ¨¡å¼) ...")
        
        try:
            # === ğŸ”¥ è°ƒç”¨ç®¡é“ ===
            # è¿™é‡Œè°ƒç”¨ src/pipelines.pyï¼Œå®ƒä¼šé€ä¼  posts_to_get ç»™ä½ çš„åº•å±‚ä»£ç 
            df = top_posts_subreddit_pipeline(
                subreddit_name=subreddit,
                post_limit=POST_LIMIT,
                comment_limmit=COMMENT_LIMIT,
                posts_to_get="Hot"  # <--- å…³é”®ï¼šä½¿ç”¨ä½ æ–°åŠ çš„ Hot æ¨¡å¼
            )
            
            if df.empty:
                logger.warning(f"âš ï¸ r/{subreddit} æ²¡æŠ“åˆ°æ•°æ®")
                continue

            dist = calculate_distribution(df)
            top_words_raw = get_top_n_words(df, 'clean_title', n=5)
            top_keywords = [word for word, count in top_words_raw]

            raw_samples = []
            for _, row in df.head(5).iterrows():
                # åªæœ‰å½“ keywords å­˜åœ¨æ—¶æ‰å»åŒ¹é…é«˜äº®
                if keywords:
                    matched_entities = [k for k in keywords if k.lower() in row['title'].lower()]
                else:
                    matched_entities = []
                
                raw_samples.append({
                    "title": row['title'],
                    "sentiment_label": row.get('sentiment_clean_title_label', 'unknown').upper(),
                    "sentiment_score": round(row.get('sentiment_clean_title_score', 0), 3),
                    "key_entities": matched_entities if matched_entities else ["General"]
                })

            subreddit_report = {
                "timestamp": datetime.utcnow().isoformat(),
                "subreddit": subreddit,
                "summary": {
                    "sentiment_distribution": dist,
                    "top_keywords": top_keywords
                },
                "raw_data_sample": raw_samples
            }
            
            final_report.append(subreddit_report)
            logger.info(f"âœ… r/{subreddit} å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ r/{subreddit} å‡ºé”™: {e}")

    # 3. å­˜æ•°æ®
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=4, ensure_ascii=False)
    
    logger.info(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼Œç»“æœå·²ä¿å­˜è‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    run_mission()
