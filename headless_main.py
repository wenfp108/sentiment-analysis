import os
import json
import base64
import requests
import numpy as np
from datetime import datetime, timezone, timedelta

# å¼•å…¥é¡¹ç›®æ¨¡å—
from src.pipelines import top_posts_subreddit_pipeline
from src.logger_config import setup_logger

logger = setup_logger()

# === é…ç½®åŒº ===
COMMAND_REPO = "wenfp108/Central-Bank"
OUTPUT_ROOT = "reddit/sentiment" # å­˜åˆ° Central Bank çš„å“ªä¸ªæ–‡ä»¶å¤¹
POOL_SIZE = 10     # æŠ“æ¯ä¸ªè®ºå›çš„å‰ 10 è´´
COMMENT_LIMIT = 5  # (æ­¤å‚æ•°åœ¨ get_reddit_data å†…éƒ¨å·²è¢«å›ºå®šä¸º 3ï¼Œä½†éœ€ä¿ç•™ä¼ å‚)

def get_github_headers():
    token = os.environ.get("GITHUB_TOKEN") # å¿…é¡»åœ¨ Action Secrets é‡Œé…å¥½
    if not token:
        logger.error("âŒ GITHUB_TOKEN not found!")
        return None
    return {
        "Authorization": f"Bearer {token}",
        "Accept": "application/vnd.github.v3+json"
    }

def fetch_missions():
    """å» Central-Bank çš„ Issue åŒºæ‰¾ä»»åŠ¡"""
    headers = get_github_headers()
    if not headers: return {}
    
    try:
        url = f"https://api.github.com/repos/{COMMAND_REPO}/issues?state=open"
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code != 200: return {}
        
        missions = {}
        for issue in resp.json():
            title = issue.get('title', '').lower()
            # è¯†åˆ«æ ‡é¢˜å¸¦ [reddit] çš„ Issue
            if '[reddit]' in title:
                # æå– Body é‡Œçš„å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”
                sub_name = title.replace('[reddit]', '').strip()
                keywords = issue.get('body', '').strip().split(',') if issue.get('body') else []
                missions[sub_name] = keywords
        return missions
    except Exception as e:
        logger.error(f"Fetch missions failed: {e}")
        return {}

def sync_to_central_bank(data_batch):
    """æŠŠç»“æœä½œä¸ºå¢é‡æ–‡ä»¶å­˜å› Central-Bank"""
    headers = get_github_headers()
    if not headers: return

    # === ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šç”Ÿæˆå”¯ä¸€çš„æ—¶é—´æˆ³æ–‡ä»¶å ===
    # æ ¼å¼ï¼šreddit/sentiment/2026/02/04/120000.json
    now = datetime.now(timezone(timedelta(hours=8)))
    date_path = now.strftime('%Y/%m/%d')
    time_str = now.strftime('%H%M%S')
    
    path = f"{OUTPUT_ROOT}/{date_path}/{time_str}.json"
    api_url = f"https://api.github.com/repos/{COMMAND_REPO}/contents/{path}"
    
    # ç›´æ¥ Push (ä¸Šä¼ )ï¼Œä¸éœ€è¦ Pull (æ‹‰å–æ—§æ•°æ®)
    try:
        # å°†æœ¬æ¬¡æ•°æ®åŒ…è½¬ä¸º JSON åˆ—è¡¨æ ¼å¼ï¼Œæ–¹ä¾¿ Refinery ç»Ÿä¸€å¤„ç†
        final_content = json.dumps([data_batch], indent=2, ensure_ascii=False)
        b64_content = base64.b64encode(final_content.encode('utf-8')).decode('utf-8')
        
        payload = {
            "message": f"ğŸ¤– Reddit Incremental: {now.strftime('%H:%M:%S')}",
            "content": b64_content,
            "branch": "main"
        }
        
        resp = requests.put(api_url, headers=headers, json=payload)
        if resp.status_code in [200, 201]:
            logger.info(f"âœ… Data synced to {path}")
        else:
            logger.error(f"âŒ Upload failed: {resp.status_code} {resp.text}")
            
    except Exception as e:
        logger.error(f"Sync failed: {e}")

def run():
    # 1. é¢†ä»»åŠ¡
    missions = fetch_missions()
    if not missions:
        logger.info("ğŸ’¤ No missions found in Issues.")
        return
        
    logger.info(f"ğŸ›¡ï¸ Missions accepted: {list(missions.keys())}")
    
    batch_results = []
    
    # 2. æ‰§è¡Œä»»åŠ¡
    for sub, keywords in missions.items():
        try:
            # è°ƒç”¨ Pipeline
            df = top_posts_subreddit_pipeline(sub, POOL_SIZE, COMMENT_LIMIT, "Hot")
            if df.empty: continue
            
            # é€‰å‡º Champion (å¾—åˆ†æœ€é«˜çš„ 5 ä¸ª)
            # rank_score = åŸºç¡€çƒ­åº¦(score) * æƒ…ç»ªå¼ºåº¦(abs(vibe) + 0.1)
            # å¢åŠ  0.1 æ˜¯ä¸ºäº†é˜²æ­¢ vibe ä¸º 0 æ—¶ score è¢«æŠ¹å¹³
            df['rank_score'] = df['score'] * (df['vibe_val'].abs() + 0.1)
            champions = df.sort_values('rank_score', ascending=False).head(5)
            
            post_list = []
            for _, row in champions.iterrows():
                post_list.append({
                    "title": row['title'],
                    "url": row['url'],
                    "score": int(row['score']),
                    "vibe": float(row['vibe_val']), # æƒ…ç»ªåˆ†
                    "summary": row['clean_text'][:100] # æ‘˜è¦
                })
            
            batch_results.append({
                "subreddit": sub,
                "avg_sentiment": float(df['vibe_val'].mean()),
                "champions": post_list
            })
            
        except Exception as e:
            logger.error(f"Failed to process r/{sub}: {e}")
            
    # 3. ä¸Šä¼ ç»“æœ
    if batch_results:
        payload = {
            "timestamp": datetime.now(timezone(timedelta(hours=8))).isoformat(),
            "data": batch_results
        }
        sync_to_central_bank(payload)
    else:
        logger.info("âš ï¸ No data fetched this run.")

if __name__ == "__main__":
    run()
