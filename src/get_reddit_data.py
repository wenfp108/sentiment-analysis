import time
import requests
import random
from .logger_config import setup_logger

logger = setup_logger()

# === ğŸ›¡ï¸ è‡ªåŠ¨è·å–é•œåƒåˆ—è¡¨ ===
def get_fresh_mirrors():
    """ä»å®˜æ–¹ä»“åº“åŠ¨æ€è·å–æ´»è·ƒçš„ RedLib å®ä¾‹"""
    fallback_mirrors = [
        'https://redlib.privacyredirect.com',
        'https://libreddit.bus-hit.me',
        'https://redlib.perennialteks.com',
        'https://redlib.freedit.eu',
        'https://www.reddit.com' # æœ€åçš„å…œåº•
    ]
    
    try:
        # è¿™æ˜¯ä¸€ä¸ªç»´æŠ¤å¾—å¾ˆå¥½çš„å®ä¾‹åˆ—è¡¨ JSON
        url = "https://raw.githubusercontent.com/redlib-org/redlib-instances/main/instances.json"
        resp = requests.get(url, timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            # ç­›é€‰å‡º up (åœ¨çº¿) ä¸” monitor (è¢«ç›‘æ§) çš„å®ä¾‹
            fresh_list = [inst['url'] for inst in data if inst.get('monitor', {}).get('status') == 'up']
            
            if fresh_list:
                # éšæœºé€‰ 5 ä¸ªï¼Œé˜²æ­¢åªç›¯ç€ä¸€ä¸ªè–…
                selected = random.sample(fresh_list, min(5, len(fresh_list)))
                # å¿…é¡»æŠŠå®˜æ–¹æºåŠ åœ¨æœ€åä½œä¸ºå…œåº•
                selected.append('https://www.reddit.com')
                logger.info(f"ğŸ”„ Refreshed mirrors: {len(selected)} active instances found.")
                return selected
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to fetch dynamic mirrors: {e}")
    
    return fallback_mirrors

# åˆå§‹åŒ–æ—¶è·å–ä¸€æ¬¡å³å¯
MIRRORS = get_fresh_mirrors()

def fetch_json(path):
    """é€šç”¨è¯·æ±‚å‡½æ•°"""
    # ä¼ªè£… Header
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    }

    for mirror in MIRRORS:
        try:
            url = f"{mirror}{path}"
            
            # å®˜æ–¹æºç‰¹æ®Šå¤„ç†
            if 'reddit.com' in mirror:
                time.sleep(2) # å®˜æ–¹æºå¿…é¡»æ…¢ä¸€ç‚¹
            else:
                separator = '&' if '?' in url else '?'
                url += f"{separator}t={int(time.time())}"

            resp = requests.get(url, headers=headers, timeout=10)
            
            if resp.status_code == 200:
                try:
                    return resp.json()
                except: continue
            elif resp.status_code == 429:
                time.sleep(2)
            
        except:
            continue
            
    logger.error(f"âŒ All mirrors failed for path: {path}")
    return None

def get_top_comments_text(post_id):
    """è·å– Top 3 è¯„è®º"""
    data = fetch_json(f"/comments/{post_id}.json")
    comments_list = []
    
    if data and isinstance(data, list) and len(data) > 1:
        try:
            children = data[1].get('data', {}).get('children', [])
            for child in children[:3]:
                body = child.get('data', {}).get('body')
                if body and body not in ['[deleted]', '[removed]']:
                    # ç®€å•æ¸…æ´—ä¸€ä¸‹æ¢è¡Œ
                    clean_body = body.replace('\n', ' ').strip()
                    comments_list.append(clean_body)
        except: pass
    
    return " | ".join(comments_list)

def get_post_data(subreddit_name, post_limit=10, comment_limmit=5, reddit=None, posts_to_get="Hot"):
    # å¼ºåˆ¶ä½¿ç”¨ .json
    list_path = f"/r/{subreddit_name}/hot.json?limit={post_limit}"
    
    logger.info(f"ğŸš€ [Fetch] r/{subreddit_name} via pool ({len(MIRRORS)} nodes)...")
    list_data = fetch_json(list_path)
    
    cleaned_posts = []
    
    if list_data and isinstance(list_data, dict) and 'data' in list_data:
        children = list_data['data'].get('children', [])
        
        for i, child in enumerate(children):
            try:
                p = child['data']
                pid = p['id']
                
                # æŠ“å–è¯„è®º
                time.sleep(1)
                comments_text = get_top_comments_text(pid)
                
                post_obj = {
                    "title": p.get('title'),
                    "id": pid,
                    "url": f"https://www.reddit.com{p.get('permalink')}",
                    "score": p.get('score', 0),
                    "upvote_ratio": p.get('upvote_ratio', 1.0),
                    "num_comments": p.get('num_comments', 0),
                    "created_utc": p.get('created_utc'),
                    "subreddit": subreddit_name,
                    # ä½ çš„æ ¸å¿ƒéœ€æ±‚ï¼šæ ‡é¢˜+è¯„è®ºæ‹¼æ¥
                    "selftext": f"{p.get('title')} . Top Comments: {comments_text}",
                    "comments": [] 
                }
                cleaned_posts.append(post_obj)
            except: continue
            
    return cleaned_posts
