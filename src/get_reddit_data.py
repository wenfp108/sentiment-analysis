import time
import requests
import random
from .logger_config import setup_logger

logger = setup_logger()

# === ğŸ›¡ï¸ é•œåƒç«™æ±  (å·²æ›´æ–°ä¸º 2026 æ´»è·ƒèŠ‚ç‚¹) ===
# ç­–ç•¥ï¼šæ··åˆä½¿ç”¨ RedLib å®ä¾‹å’Œå®˜æ–¹æº
MIRRORS = [
    # ä¼˜å…ˆå°è¯•æ¯”è¾ƒç¨³å®šçš„ç§æœ‰/å°ä¼—å®ä¾‹
    'https://redlib.privacyredirect.com',
    'https://libreddit.bus-hit.me',
    'https://redlib.perennialteks.com',
    'https://redlib.freedit.eu',
    'https://libreddit.kavin.rocks',
    # æœ€åçš„å…œåº•ï¼šå®˜æ–¹æº (è™½ç„¶å®¹æ˜“é™æµï¼Œä½†æ¯”æ­»æ‰å¥½)
    'https://www.reddit.com'
]

def fetch_json(path):
    """é€šç”¨è¯·æ±‚å‡½æ•°ï¼šè½®è¯¢é•œåƒç«™ç›´åˆ°æˆåŠŸ"""
    headers = {
        # ä¼ªè£…æˆçœŸå®çš„æ¡Œé¢æµè§ˆå™¨ï¼Œé˜²æ­¢è¢«ç§’æ€
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5'
    }

    for mirror in MIRRORS:
        try:
            url = f"{mirror}{path}"
            
            # å®˜æ–¹æºéœ€è¦ç‰¹åˆ«å¤„ç†ï¼šä¸èƒ½åŠ å¤ªé¢‘ç¹çš„éšæœºå‚ï¼Œä¸”å¿…é¡» .json ç»“å°¾
            if 'reddit.com' in mirror:
                # å®˜æ–¹æºè¯·æ±‚ç¨å¾®æ…¢ä¸€ç‚¹ï¼Œé˜²å°
                time.sleep(1)
            else:
                # é•œåƒç«™åŠ éšæœºå‚é˜²ç¼“å­˜
                separator = '&' if '?' in url else '?'
                url += f"{separator}t={int(time.time())}"

            # logger.info(f"   Trying: {mirror} ...")
            resp = requests.get(url, headers=headers, timeout=8)
            
            if resp.status_code == 200:
                try:
                    return resp.json()
                except:
                    # æœ‰æ—¶å€™è¿”å›çš„æ˜¯ HTML é”™è¯¯é¡µè€Œä¸æ˜¯ JSON
                    continue
            elif resp.status_code == 429:
                logger.warning(f"   âš ï¸ Rate Limit (429) at {mirror}")
                time.sleep(2) # é‡åˆ°é™æµç¨å¾®æ­‡ä¸€ä¸‹
            
        except Exception as e:
            # logger.warning(f"   âŒ Error {mirror}: {e}")
            continue
            
    logger.error(f"âŒ All mirrors failed for path: {path}")
    return None

def get_top_comments_text(post_id):
    """è·å– Top 3 è¯„è®ºæ–‡æœ¬"""
    # è¯„è®ºåŒºåªå°è¯•ä¸€æ¬¡ï¼Œä¸éœ€è¦å¤ªé‡
    data = fetch_json(f"/comments/{post_id}.json")
    comments_list = []
    
    if data and isinstance(data, list) and len(data) > 1:
        try:
            children = data[1].get('data', {}).get('children', [])
            for child in children[:3]: # åªå–å‰3æ¡
                body = child.get('data', {}).get('body')
                score = child.get('data', {}).get('score', 0)
                if body and body not in ['[deleted]', '[removed]']:
                    comments_list.append(f"[Score:{score}] {body}")
        except: pass
    
    return " | ".join(comments_list)

def get_post_data(subreddit_name, post_limit=10, comment_limmit=5, reddit=None, posts_to_get="Hot"):
    logger.info(f"ğŸš€ [Fetch] r/{subreddit_name} ({posts_to_get})...")
    
    # å¼ºåˆ¶ä½¿ç”¨ .json åç¼€ï¼Œè¿™å¯¹å®˜æ–¹æºå’Œé•œåƒç«™éƒ½é€‚ç”¨
    # æ³¨æ„ï¼šLimit å‚æ•°åœ¨ URL é‡Œ
    list_path = f"/r/{subreddit_name}/hot.json?limit={post_limit}"
    list_data = fetch_json(list_path)
    
    cleaned_posts = []
    
    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§ (RedLib å’Œ Reddit åŸç”Ÿ JSON ç»“æ„ç•¥æœ‰ä¸åŒï¼Œä½† data.children æ˜¯ä¸€æ ·çš„)
    if list_data and isinstance(list_data, dict) and 'data' in list_data:
        children = list_data['data'].get('children', [])
        
        for i, child in enumerate(children):
            try:
                p = child['data']
                pid = p['id']
                
                # æŠ“å–è¯„è®º (ä¼‘çœ é˜²å°)
                time.sleep(1.5) 
                comments_text = get_top_comments_text(pid)
                
                post_obj = {
                    "title": p.get('title'),
                    "id": pid,
                    "url": f"https://www.reddit.com{p.get('permalink')}",
                    "score": p.get('score', 0),
                    "upvote_ratio": p.get('upvote_ratio', 1.0),
                    "num_comments": p.get('num_comments', 0),
                    "created_utc": p.get('created_utc'),
                    "subreddit": subreddit_name,
                    "selftext": f"{p.get('title')} . Top Comments: {comments_text}",
                    "comments": [] 
                }
                cleaned_posts.append(post_obj)
            except Exception as e:
                logger.error(f"Error parsing post: {e}")
                continue
            
    return cleaned_posts
