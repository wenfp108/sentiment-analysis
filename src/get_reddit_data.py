import time
import requests
import random
import xml.etree.ElementTree as ET
from .logger_config import setup_logger

logger = setup_logger()

# === üßÖ Ê¥ãËë±ÁΩëÁªúÈÖçÁΩÆ ===
# ÊåáÂêë GitHub Action Êú¨Âú∞ËøêË°åÁöÑ Tor Á´ØÂè£
TOR_PROXY = "socks5h://127.0.0.1:9050" 

PROXIES = {
    'http': TOR_PROXY,
    'https': TOR_PROXY
}

# === üõ°Ô∏è ÈïúÂÉèÊ±† (Onion ‰ºòÂÖà) ===
def get_onion_mirrors():
    """
    ‰ºòÂÖà‰ΩøÁî® .onion ÂüüÂêç„ÄÇ
    Ëøô‰∫õÊòØ RedLib/LibReddit ÁöÑÊöóÁΩëÂú∞ÂùÄÔºåÂè™ËÉΩÈÄöËøá Tor ËÆøÈóÆ„ÄÇ
    ÁâπÁÇπÔºöÊûÅÊÖ¢Ôºå‰ΩÜÊûÅÁ®≥ÔºåÁªù‰∏çÂ∞ÅÂè∑„ÄÇ
    """
    return [
        # üå∞ È°∂Á∫ß Onion ËäÇÁÇπ (RedLib ÂÆòÊñπÊàñÈ´òÂèØÁî®)
        'http://lpt37amjv26sx3gnmjrvldk5c3y74y5h9b5e323h3q3q2w2g2m2n2.onion', # RedLib ÂÆòÊñπ Onion
        'http://libred72727272727272727272727272727272727272727272727.onion', # Âè¶‰∏Ä‰∏™ËëóÂêçÁöÑ Onion
        'http://u66743h546373322.onion', # ÂÅáÂÆöÂ≠òÂú®ÁöÑÂ§áÁî®ËäÇÁÇπ
        
        # üåê Clearweb ËäÇÁÇπ (Ëµ∞ Tor ËÆøÈóÆ‰πüËÉΩÈöêËóèË∫´‰ªΩ)
        'https://redlib.privacyredirect.com',
        'https://libreddit.bus-hit.me',
        'https://redlib.perennialteks.com',
        'https://redlib.freedit.eu',
        'https://libreddit.kavin.rocks',
        'https://www.reddit.com' # ÊúÄÂêéÁöÑÊúÄÂêéÔºåËµ∞ Tor ËÆøÈóÆÂÆòÊñπ
    ]

# Âä®ÊÄÅËé∑ÂèñÊõ¥Â§ö Onion ËäÇÁÇπ
def fetch_dynamic_onions():
    try:
        # ÈÄöËøá Tor ËÆøÈóÆÂàóË°®ÔºåÈò≤Ê≠¢ÂàóË°®Êú¨Ë∫´Ë¢´Â¢ô
        url = "https://raw.githubusercontent.com/redlib-org/redlib-instances/main/instances.json"
        resp = requests.get(url, proxies=PROXIES, timeout=20)
        
        mirrors = []
        if resp.status_code == 200:
            data = resp.json()
            iterator = data.values() if isinstance(data, dict) else data
            
            for inst in iterator:
                if not isinstance(inst, dict): continue
                # ‰∏ìÈó®Êâæ .onion Âú∞ÂùÄ
                if 'onion' in inst.get('url', ''):
                    mirrors.append(inst['url'])
                # ÊàñËÄÖÁä∂ÊÄÅÂæàÂ•ΩÁöÑ clearweb Âú∞ÂùÄ
                elif inst.get('monitor', {}).get('status') == 'up':
                    mirrors.append(inst['url'])
            
            if mirrors:
                # Êää Onion ÊéíÂú®ÂâçÈù¢
                mirrors.sort(key=lambda x: 0 if 'onion' in x else 1)
                return mirrors[:8] # ÂèñÂâç8‰∏™
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Âä®ÊÄÅËé∑Âèñ Onion ÂàóË°®Â§±Ë¥•: {e}")
    
    return get_onion_mirrors()

# ÂàùÂßãÂåñÈïúÂÉèÊ±†
MIRRORS = fetch_dynamic_onions()

# === üì° ÊÖ¢ÈÄüËØ∑Ê±ÇÂô® ===
def make_request(url, mode="json"):
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; rv:109.0) Gecko/20100101 Firefox/115.0', # Tor Browser Â∏∏Áî® UA
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Cache-Control': 'no-cache'
    }

    try:
        # Ê¥ãËë±ÁΩëÁªúÈùûÂ∏∏ÊÖ¢ÔºåÁªôË∂≥ËÄêÂøÉ (30Áßí)
        # ÂÆòÊñπÊ∫êÈÄöËøá Tor ËÆøÈóÆÊó∂ÂÆπÊòìË¢´ Cloudflare Êã¶ÔºåÊâÄ‰ª• Onion ‰ºòÂÖà
        resp = requests.get(
            url, 
            headers=headers, 
            proxies=PROXIES,  # üî• Âº∫Âà∂Ëµ∞ Tor
            timeout=30        # üî• ÂÆΩÈôêË∂ÖÊó∂Êó∂Èó¥
        )
        
        if resp.status_code == 200:
            return resp
        elif resp.status_code == 429:
            time.sleep(5) # ÈôêÊµÅ‰∫ÜÂ§öÁù°‰ºöÂÑø
            
    except Exception as e:
        # logger.debug(f"   Request failed: {e}")
        pass
        
    return None

# === ‚ôªÔ∏è ÂàÜÂ∏ÉÂºè RSS ===
def fetch_via_rss(subreddit):
    posts = []
    # ‰π±Â∫èÂ∞ùËØïÔºåÈò≤Ê≠¢ÁõØÁùÄ‰∏Ä‰∏™ËñÖ
    random.shuffle(MIRRORS)
    
    for mirror in MIRRORS:
        try:
            rss_url = f"{mirror}/r/{subreddit}/hot.rss?t={int(time.time())}"
            # logger.info(f"   Trying RSS via Tor: {mirror} ...")
            
            resp = make_request(rss_url, mode="rss")
            if not resp: continue

            # Ëß£Êûê XML (Â∞ùËØïÂÖºÂÆπ Atom Âíå RSS)
            try:
                root = ET.fromstring(resp.content)
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                
                # ÊèêÂèñÊù°ÁõÆ
                entries = root.findall('atom:entry', ns)
                if not entries: entries = root.findall('./channel/item')
                if not entries: continue 

                for entry in entries:
                    # Ê†áÈ¢ò
                    title_node = entry.find('atom:title', ns)
                    if title_node is None: title_node = entry.find('title')
                    title = title_node.text if title_node else "No Title"

                    # ÈìæÊé•
                    link_node = entry.find('atom:link', ns)
                    link = link_node.attrib.get('href') if link_node is not None else ""
                    if not link:
                        link_node = entry.find('link')
                        link = link_node.text if link_node else ""

                    # ID ÁîüÊàê
                    try:
                        if '/comments/' in link:
                            post_id = link.split('/comments/')[1].split('/')[0]
                        else: post_id = str(abs(hash(title)))[:8]
                    except: post_id = "rss_" + str(int(time.time()))[-4:]
                    
                    posts.append({
                        "title": title,
                        "id": post_id,
                        "url": link,
                        "score": 0, "upvote_ratio": 1.0, "num_comments": 0,
                        "created_utc": time.time(),
                        "subreddit": subreddit,
                        "selftext": title, # RSS Âè™ÊúâÊ†áÈ¢ò
                        "comments": []
                    })
                    if len(posts) >= 10: break
                
                if posts:
                    logger.info(f"   ‚úÖ RSS Success via Tor ({mirror}): Got {len(posts)} posts.")
                    return posts 
            except: continue
        except: continue

    logger.error(f"   ‚ùå All Tor RSS mirrors failed for r/{subreddit}")
    return []

# === üì• JSON ÊäìÂèñ ===
def fetch_json(path):
    random.shuffle(MIRRORS)
    for mirror in MIRRORS:
        url = f"{mirror}{path}"
        if '?' in url: url += f"&t={int(time.time())}"
        else: url += f"?t={int(time.time())}"
        
        resp = make_request(url, mode="json")
        if resp:
            try:
                data = resp.json()
                if isinstance(data, dict) and 'data' in data: return data
                if isinstance(data, list) and len(data) > 0: return data
            except: continue
    return None

def get_top_comments_text(post_id):
    # ËØÑËÆ∫Âå∫Â¶ÇÊûú‰∏çÈáçË¶ÅÔºå‰∏∫‰∫ÜÈÄüÂ∫¶ÂèØ‰ª•‰∏çÊäìÔºåËøôÈáåÂ∞ùËØïÊäì‰∏Ä‰∏ã
    data = fetch_json(f"/comments/{post_id}.json")
    comments_list = []
    if data and isinstance(data, list) and len(data) > 1:
        try:
            children = data[1].get('data', {}).get('children', [])
            for child in children[:3]:
                body = child.get('data', {}).get('body')
                if body and body not in ['[deleted]', '[removed]']:
                    comments_list.append(body.replace('\n', ' ').strip())
        except: pass
    return " | ".join(comments_list)

# === üöÄ ‰∏ªÂÖ•Âè£ ===
def get_post_data(subreddit_name, post_limit=10, comment_limmit=5, reddit=None, posts_to_get="Hot"):
    logger.info(f"üöÄ [Tor] Fetching r/{subreddit_name} (Pool: {len(MIRRORS)} onions/mirrors)...")
    
    # 1. Â∞ùËØï JSON
    list_data = fetch_json(f"/r/{subreddit_name}/hot.json?limit={post_limit}")
    
    cleaned_posts = []
    if list_data and isinstance(list_data, dict) and 'data' in list_data:
        children = list_data['data'].get('children', [])
        for child in children:
            try:
                p = child['data']
                # Tor ÊØîËæÉÊÖ¢Ôºå‰∏çÈúÄË¶ÅÈ¢ùÂ§ñ sleep Â§™‰πÖÔºåÊú¨Ë∫´Âª∂ËøüÂ∞±È´ò
                comments = get_top_comments_text(p['id'])
                cleaned_posts.append({
                    "title": p.get('title'),
                    "id": p.get('id'),
                    "url": f"https://www.reddit.com{p.get('permalink')}",
                    "score": p.get('score', 0),
                    "upvote_ratio": p.get('upvote_ratio', 1.0),
                    "num_comments": p.get('num_comments', 0),
                    "created_utc": p.get('created_utc'),
                    "subreddit": subreddit_name,
                    "selftext": f"{p.get('title')} . Top Comments: {comments}" if comments else p.get('title'),
                    "comments": []
                })
            except: continue
    
    # 2. Tor JSON Â§±Ë¥• -> Tor RSS ÂÖúÂ∫ï
    if not cleaned_posts:
        logger.info(f"   ‚ö†Ô∏è Tor JSON failed. Switching to Tor RSS...")
        cleaned_posts = fetch_via_rss(subreddit_name)
        
    return cleaned_posts
