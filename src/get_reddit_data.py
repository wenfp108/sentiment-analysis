import time
import requests
import random
from .logger_config import setup_logger

logger = setup_logger()

# === é•œåƒç«™æ±  ===
MIRRORS = [
    'https://l.opnxng.com',
    'https://redlib.catsarch.com', 
    'https://r.nf',
    'https://redlib.vling.net'
]

def fetch_json(path):
    """é€šç”¨é•œåƒç«™è¯·æ±‚"""
    for mirror in MIRRORS:
        try:
            url = f"{mirror}{path}"
            separator = '&' if '?' in url else '?'
            url += f"{separator}t={int(time.time())}"
            # ä¼ªè£… Header
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) RefineryBot/1.0'}
            
            # logger.info(f"Fetching: {url}")
            resp = requests.get(url, headers=headers, timeout=10)
            
            if resp.status_code == 200:
                return resp.json()
        except:
            continue
    return None

def get_top_comments_text(post_id):
    """è·å– Top 3 è¯„è®ºæ–‡æœ¬"""
    data = fetch_json(f"/comments/{post_id}.json")
    comments_list = []
    
    if data and isinstance(data, list) and len(data) > 1:
        children = data[1].get('data', {}).get('children', [])
        for child in children[:3]: # åªå–å‰3æ¡
            body = child.get('data', {}).get('body')
            score = child.get('data', {}).get('score', 0)
            if body and body not in ['[deleted]', '[removed]']:
                comments_list.append(f"[Score:{score}] {body}")
    
    return " | ".join(comments_list)

# æ³¨æ„ï¼šä¿æŒå‡½æ•°ç­¾åä¸ä½ åŸæœ‰ä»£ç å…¼å®¹ï¼Œä½†å¿½ç•¥ reddit/posts_to_get å‚æ•°
def get_post_data(subreddit_name, post_limit=10, comment_limmit=5, reddit=None, posts_to_get="Hot"):
    logger.info(f"ğŸš€ [Mirror] Fetching r/{subreddit_name} ({posts_to_get})...")
    
    # å¼ºåˆ¶ä½¿ç”¨ hot.jsonï¼Œè¿™æ˜¯æœ€ç¨³çš„æ¥å£
    list_path = f"/r/{subreddit_name}/hot.json?limit={post_limit}"
    list_data = fetch_json(list_path)
    
    cleaned_posts = []
    if list_data and 'data' in list_data:
        children = list_data['data']['children']
        
        for i, child in enumerate(children):
            p = child['data']
            pid = p['id']
            
            # ä¸ºäº†å®‰å…¨ï¼Œæ¯æŠ“ä¸€ç¯‡æ­‡ 1.5 ç§’
            time.sleep(1.5)
            # logger.info(f"   Getting comments for {pid}...")
            
            comments_text = get_top_comments_text(pid)
            
            # æ ¸å¿ƒï¼šæ„é€ ä¸åŸæœ‰ pipeline å…¼å®¹çš„å­—å…¸
            post_obj = {
                "title": p.get('title'),
                "id": pid,
                "url": f"https://www.reddit.com{p.get('permalink')}",
                "score": p.get('score', 0),
                "upvote_ratio": p.get('upvote_ratio', 1.0),
                "num_comments": p.get('num_comments', 0),
                "created_utc": p.get('created_utc'),
                "subreddit": subreddit_name,
                
                # å…³é”®ç­–ç•¥ï¼šæŠŠ æ ‡é¢˜+è¯„è®º æ‹¼å…¥ selftextï¼Œä¾› AI åˆ†æ
                "selftext": f"{p.get('title')} . Top Comments: {comments_text}",
                
                # å…¼å®¹æ—§ä»£ç ï¼šcomments è®¾ä¸ºç©ºåˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æå–äº†æ–‡æœ¬
                "comments": [] 
            }
            cleaned_posts.append(post_obj)
            
    return cleaned_posts
