import time
import requests
import random
from .logger_config import setup_logger

logger = setup_logger()

# === ğŸ›¡ï¸ é€‚åˆ Serv00 çš„é•œåƒæº (Serv00 åœ¨æ¬§æ´²ï¼Œè¿è¿™äº›æå¿«) ===
MIRRORS = [
    'https://redlib.privacyredirect.com',
    'https://redlib.freedit.eu',
    'https://redlib.perennialteks.com',
    'https://libreddit.bus-hit.me',
    'https://libreddit.kavin.rocks',
    'https://www.reddit.com' # Serv00 IP å¹²å‡€ï¼Œé€šå¸¸å¯ç›´è¿
]

def fetch_json(path):
    """
    Serv00 ä¸“ç”¨è¯·æ±‚å‡½æ•°ï¼š
    ä¸éœ€è¦ Tor ä»£ç†ï¼Œç›´æ¥ç”¨ requests è½®è¯¢
    """
    # ä¼ªè£…æˆæµè§ˆå™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    # æ‰“ä¹±é¡ºåºï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹å¼€å§‹
    random.shuffle(MIRRORS)

    for mirror in MIRRORS:
        try:
            url = f"{mirror}{path}"
            # åŠ ä¸ªéšæœºå‚æ•°é˜²ç¼“å­˜
            separator = '&' if '?' in url else '?'
            url += f"{separator}t={int(time.time())}"

            # å®˜æ–¹æºè¯·æ±‚æ…¢ä¸€ç‚¹ï¼Œé•œåƒæºå¯ä»¥å¿«ç‚¹
            timeout = 6 if 'reddit.com' not in mirror else 10
            
            resp = requests.get(url, headers=headers, timeout=timeout)
            
            if resp.status_code == 200:
                try:
                    data = resp.json()
                    # ç®€å•éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                    if isinstance(data, dict) and 'data' in data:
                        return data
                    if isinstance(data, list) and len(data) > 0:
                        return data
                except:
                    continue
            elif resp.status_code == 429:
                time.sleep(2) # é™æµäº†å°±æ­‡ä¸¤ç§’
            
        except Exception:
            continue
            
    logger.error(f"âŒ Failed to fetch {path} from all mirrors.")
    return None

def get_top_comments_text(post_id):
    """è·å–è¯„è®º"""
    data = fetch_json(f"/comments/{post_id}.json")
    comments_list = []
    
    if data and isinstance(data, list) and len(data) > 1:
        try:
            children = data[1].get('data', {}).get('children', [])
            for child in children[:3]: # å–å‰3æ¡
                body = child.get('data', {}).get('body')
                if body and body not in ['[deleted]', '[removed]']:
                    comments_list.append(body.replace('\n', ' ').strip())
        except: pass
    
    return " | ".join(comments_list)

def get_post_data(subreddit_name, post_limit=10, comment_limmit=5, reddit=None, posts_to_get="Hot"):
    logger.info(f"ğŸš€ [Serv00] Fetching r/{subreddit_name}...")
    
    # å¼ºåˆ¶åŠ ä¸Š .json
    list_path = f"/r/{subreddit_name}/hot.json?limit={post_limit}"
    list_data = fetch_json(list_path)
    
    cleaned_posts = []
    
    if list_data and isinstance(list_data, dict) and 'data' in list_data:
        children = list_data['data'].get('children', [])
        
        for i, child in enumerate(children):
            try:
                p = child['data']
                # Serv00 æ€§èƒ½ä¸€èˆ¬ï¼Œç¨å¾®ä¼‘æ¯ä¸€ä¸‹é˜²æ­¢è¯·æ±‚å¤ªå¿«è¢«å°
                time.sleep(0.2) 
                
                comments_text = get_top_comments_text(p['id'])
                
                post_obj = {
                    "title": p.get('title'),
                    "id": p.get('id'),
                    "url": f"https://www.reddit.com{p.get('permalink')}",
                    "score": p.get('score', 0),
                    "upvote_ratio": p.get('upvote_ratio', 1.0),
                    "num_comments": p.get('num_comments', 0),
                    "created_utc": p.get('created_utc'),
                    "subreddit": subreddit_name,
                    "selftext": f"{p.get('title')} . Top Comments: {comments_text}",
                    "comments": [] 
                }
                cleaned_posts.append(post_obj)
            except: continue
            
    return cleaned_posts
