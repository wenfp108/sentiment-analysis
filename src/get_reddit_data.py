import time
import requests
import random
import urllib3
from .logger_config import setup_logger

# ç¦ç”¨å®‰å…¨è­¦å‘Šï¼ˆå› ä¸ºæˆ‘ä»¬è¦å…³é—­ SSL éªŒè¯ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = setup_logger()

# === ğŸ›¡ï¸ ç»ˆæå¤‡ç”¨é•œåƒæ±  ===
# æ··åˆäº†å®˜æ–¹æ—§ç‰ˆæ¥å£ (old.reddit) å’Œ é•œåƒç«™
MIRRORS = [
    'https://old.reddit.com',            # å®˜æ–¹æ—§ç‰ˆï¼Œæœ€ç¨³ä½†æœ‰æ—¶é™æµ
    'https://www.reddit.com',            # å®˜æ–¹æ–°ç‰ˆ
    'https://redlib.privacyredirect.com',
    'https://redlib.freedit.eu',
    'https://libreddit.bus-hit.me',
]

def fetch_json(path):
    headers = {
        # ä¼ªè£…æˆ Google çˆ¬è™«æˆ–è€…éå¸¸æ™®é€šçš„æµè§ˆå™¨
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    # ç¨å¾®æ‰“ä¹±é¡ºåº
    random.shuffle(MIRRORS)

    # ä¼˜å…ˆå°è¯• old.redditï¼Œå› ä¸ºå®ƒæœ€ä¸åƒçˆ¬è™«ç›®æ ‡
    current_mirrors = sorted(MIRRORS, key=lambda x: 'old.reddit' not in x)

    for mirror in current_mirrors:
        try:
            url = f"{mirror}{path}"
            separator = '&' if '?' in url else '?'
            url += f"{separator}t={int(time.time())}"
            
            # å®˜æ–¹æºç»™é•¿ä¸€ç‚¹æ—¶é—´
            timeout = 10 if 'reddit.com' in mirror else 5
            
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šverify=False (å¿½ç•¥ SSL è¯ä¹¦é”™è¯¯)
            resp = requests.get(url, headers=headers, timeout=timeout, verify=False)
            
            if resp.status_code == 200:
                try:
                    data = resp.json()
                    if (isinstance(data, dict) and 'data' in data) or (isinstance(data, list) and len(data) > 0):
                        return data
                except:
                    pass
            elif resp.status_code == 429:
                time.sleep(2) # è¢«é™æµäº†ï¼Œæ­‡ä¼šå„¿
            else:
                # æ‰“å°å…·ä½“é”™è¯¯ç ï¼Œæ–¹ä¾¿è°ƒè¯•
                logger.warning(f"âš ï¸ {mirror} returned {resp.status_code}")
                
        except Exception as e:
            # æ‰“å°å…·ä½“æŠ¥é”™åŸå› ï¼
            logger.warning(f"âš ï¸ Connect {mirror} failed: {str(e)[:50]}")
            continue
            
    logger.error(f"âŒ Failed to fetch {path} from all mirrors.")
    return None

def get_top_comments_text(post_id):
    data = fetch_json(f"/comments/{post_id}.json")
    comments_list = []
    if data and isinstance(data, list) and len(data) > 1:
        try:
            children = data[1].get('data', {}).get('children', [])
            for child in children[:3]:
                body = child.get('data', {}).get('body')
                if body and body not in ['[deleted]', '[removed]']:
                    comments_list.append(body.replace('\n', ' ').strip())
        except: pass
    return " | ".join(comments_list)

def get_post_data(subreddit_name, post_limit=10, comment_limmit=5, reddit=None, posts_to_get="Hot"):
    logger.info(f"ğŸš€ [Serv00] Fetching r/{subreddit_name}...")
    
    # URL ä¿®æ­£
    list_path = f"/r/{subreddit_name}/{posts_to_get.lower()}.json?limit={post_limit}"
    list_data = fetch_json(list_path)
    
    cleaned_posts = []
    
    if list_data and isinstance(list_data, dict) and 'data' in list_data:
        children = list_data['data'].get('children', [])
        for child in children:
            try:
                p = child['data']
                time.sleep(0.5) # ç¨å¾®å¿«ä¸€ç‚¹
                
                # å¦‚æœè¦å®Œæ•´è¯„è®ºï¼Œå–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Šï¼ˆä¼šå˜æ…¢ï¼‰
                # comments = get_top_comments_text(p['id'])
                comments = ""
                
                cleaned_posts.append({
                    "title": p.get('title'),
                    "id": p.get('id'),
                    "url": f"https://www.reddit.com{p.get('permalink')}",
                    "score": p.get('score', 0),
                    "upvote_ratio": p.get('upvote_ratio', 1.0),
                    "num_comments": p.get('num_comments', 0),
                    "created_utc": p.get('created_utc'),
                    "subreddit": subreddit_name,
                    "selftext": f"{p.get('title')} . {p.get('selftext', '')[:200]}",
                    "comments": []
                })
            except: continue
                
    return cleaned_posts
