import time
import requests
import random
from .logger_config import setup_logger

logger = setup_logger()

# === ğŸ›¡ï¸ 1. è‡ªåŠ¨çŒæ€æ´»è·ƒé•œåƒ (ä¿®å¤ç‰ˆ) ===
def get_fresh_mirrors():
    """åŠ¨æ€è·å–æ´»è·ƒçš„ RedLib å®ä¾‹ï¼Œå…¼å®¹å¤šç§ JSON æ ¼å¼"""
    # å¤‡é€‰æ± ï¼šæ··åˆäº† RedLib å’Œ Reddit å®˜æ–¹æº
    fallback_mirrors = [
        'https://redlib.privacyredirect.com',
        'https://libreddit.bus-hit.me',
        'https://redlib.perennialteks.com',
        'https://redlib.freedit.eu',
        'https://libreddit.kavin.rocks',
        'https://snoo.habedieeh.re',
        'https://www.reddit.com', # å®˜æ–¹æº
        'https://old.reddit.com'  # è€ç‰ˆå®˜æ–¹æº (æœ‰æ—¶å€™é™åˆ¶å®½æ¾ç‚¹)
    ]
    
    try:
        logger.info("ğŸ”„ æ­£åœ¨ä»å®˜æ–¹åˆ—è¡¨å¯»æ‰¾æ´»è·ƒèŠ‚ç‚¹...")
        url = "https://raw.githubusercontent.com/redlib-org/redlib-instances/main/instances.json"
        resp = requests.get(url, timeout=10)
        
        if resp.status_code == 200:
            data = resp.json()
            fresh_list = []

            # å…¼å®¹å¤„ç†ï¼šå¦‚æœæ˜¯å­—å…¸ï¼Œå– valuesï¼›å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥ç”¨
            iterator = data.values() if isinstance(data, dict) else data

            for inst in iterator:
                # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šç¡®ä¿ inst æ˜¯å­—å…¸ä¸”æœ‰ url
                if not isinstance(inst, dict): continue
                
                # ç­›é€‰æ ‡å‡†ï¼šstatus=up ä¸” monitor=trueï¼Œä¸”ä¸æ˜¯æš—ç½‘(.onion)
                monitor = inst.get('monitor', {})
                if monitor.get('status') == 'up' and 'onion' not in inst.get('url', ''):
                    fresh_list.append(inst['url'])
            
            if fresh_list:
                # éšæœºé€‰ 6 ä¸ªæ–°èŠ‚ç‚¹ + å®˜æ–¹æº
                selected = random.sample(fresh_list, min(6, len(fresh_list)))
                selected.append('https://www.reddit.com')
                logger.info(f"âœ… æˆåŠŸé”å®š {len(selected)} ä¸ªæ´»è·ƒèŠ‚ç‚¹ï¼")
                return selected
            else:
                logger.warning("âš ï¸ è·å–åˆ°çš„åˆ—è¡¨ä¸ºç©ºï¼Œåˆ‡æ¢å›å¤‡é€‰æ± ã€‚")
    except Exception as e:
        logger.warning(f"âš ï¸ åŠ¨æ€è·å–å¤±è´¥ ({type(e).__name__})ï¼Œåˆ‡æ¢å›å¤‡é€‰æ± : {e}")
    
    return fallback_mirrors

# åˆå§‹åŒ–
MIRRORS = get_fresh_mirrors()

def fetch_json(path):
    """é€šç”¨è¯·æ±‚å‡½æ•° (å¸¦è¯¦ç»†è¯Šæ–­)"""
    # ğŸ­ ä¼ªè£…æˆæœ€æ–°çš„ Chrome
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
    }

    for mirror in MIRRORS:
        try:
            url = f"{mirror}{path}"
            
            # å®˜æ–¹æºå¿…é¡»æ…¢ä¸€ç‚¹ï¼Œä¸”ä¸èƒ½å¸¦éšæœºå‚æ•°
            if 'reddit.com' in mirror:
                time.sleep(2) 
            else:
                separator = '&' if '?' in url else '?'
                url += f"{separator}t={int(time.time())}"

            # logger.info(f"   Trying: {mirror} ...")
            resp = requests.get(url, headers=headers, timeout=8)
            
            if resp.status_code == 200:
                try:
                    data = resp.json()
                    # å†æ¬¡ç¡®è®¤æ•°æ®æœ‰æ•ˆæ€§
                    if isinstance(data, dict) and 'data' in data:
                        return data
                    if isinstance(data, list) and len(data) > 0:
                        return data
                    # å¦‚æœè¿”å›ç©ºå­—å…¸æˆ–æ— å…³å†…å®¹
                    # logger.warning(f"   âš ï¸ {mirror} returned valid 200 but invalid JSON structure.")
                except: 
                    pass # JSON è§£æå¤±è´¥ï¼Œè·³è¿‡
            elif resp.status_code == 429:
                # logger.warning(f"   âš ï¸ Rate Limit (429) at {mirror}")
                time.sleep(2)
            else:
                # æ‰“å°é”™è¯¯ç ï¼Œæ–¹ä¾¿è°ƒè¯•
                # logger.warning(f"   âŒ {mirror} failed with {resp.status_code}")
                pass
            
        except requests.exceptions.Timeout:
            # logger.warning(f"   â³ Timeout at {mirror}")
            continue
        except Exception as e:
            # logger.warning(f"   âŒ Error at {mirror}: {e}")
            continue
            
    logger.error(f"âŒ æ‰€æœ‰èŠ‚ç‚¹ ({len(MIRRORS)}ä¸ª) å…¨éƒ¨å°è¯•å¤±è´¥: {path}")
    return None

def get_top_comments_text(post_id):
    """è·å– Top 3 è¯„è®º"""
    data = fetch_json(f"/comments/{post_id}.json")
    comments_list = []
    
    if data and isinstance(data, list) and len(data) > 1:
        try:
            children = data[1].get('data', {}).get('children', [])
            for child in children[:3]:
                body = child.get('data', {}).get('body')
                if body and body not in ['[deleted]', '[removed]']:
                    clean = body.replace('\n', ' ').strip()
                    comments_list.append(clean)
        except: pass
    
    return " | ".join(comments_list)

def get_post_data(subreddit_name, post_limit=10, comment_limmit=5, reddit=None, posts_to_get="Hot"):
    logger.info(f"ğŸš€ [Fetch] r/{subreddit_name} (Pool: {len(MIRRORS)} nodes)...")
    
    # å¼ºåˆ¶åŠ ä¸Š .json
    list_path = f"/r/{subreddit_name}/hot.json?limit={post_limit}"
    list_data = fetch_json(list_path)
    
    cleaned_posts = []
    
    if list_data and isinstance(list_data, dict) and 'data' in list_data:
        children = list_data['data'].get('children', [])
        
        for i, child in enumerate(children):
            try:
                p = child['data']
                pid = p['id']
                
                time.sleep(1) # ç¨å¾®æ…¢ç‚¹ï¼Œç¨³ä¸€ç‚¹
                comments_text = get_top_comments_text(pid)
                
                post_obj = {
                    "title": p.get('title'),
                    "id": pid,
                    "url": f"https://www.reddit.com{p.get('permalink')}",
                    "score": p.get('score', 0),
                    "upvote_ratio": p.get('upvote_ratio', 1.0),
                    "num_comments": p.get('num_comments', 0),
                    "created_utc": p.get('created_utc'),
                    "subreddit": subreddit_name,
                    "selftext": f"{p.get('title')} . Top Comments: {comments_text}",
                    "comments": [] 
                }
                cleaned_posts.append(post_obj)
            except: continue
            
    return cleaned_posts
