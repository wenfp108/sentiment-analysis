import time
import requests
import random
import xml.etree.ElementTree as ET
from datetime import datetime, timezone
from .logger_config import setup_logger

logger = setup_logger()

# === 1. é•œåƒç«™æ±  (ç²¾é€‰é«˜æ´»èŠ‚ç‚¹) ===
def get_fresh_mirrors():
    """å®šä¹‰ä¼˜å…ˆå°è¯•çš„é•œåƒåˆ—è¡¨"""
    return [
        'https://redlib.privacyredirect.com',
        'https://libreddit.bus-hit.me',
        'https://redlib.perennialteks.com',
        'https://redlib.freedit.eu',
        'https://libreddit.kavin.rocks',
        'https://snoo.habedieeh.re',
        'https://www.reddit.com',     # å®˜æ–¹ JSON
        'https://old.reddit.com'      # è€ç‰ˆ JSON
    ]

MIRRORS = get_fresh_mirrors()

# === 2. é€šç”¨è¯·æ±‚å‡½æ•° ===
def make_request(url, mode="json"):
    """å‘é€ HTTP è¯·æ±‚ï¼Œè‡ªåŠ¨ä¼ªè£…"""
    # è½®æ¢ User-Agentï¼Œæœ‰æ—¶å€™ä¼ªè£…æˆ FeedBurner (RSSæŠ“å–å™¨) ä¼šæœ‰å¥‡æ•ˆ
    user_agents = [
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
        'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
        'FeedBurner/1.0 (http://www.FeedBurner.com)', 
        'RefineryBot/1.0'
    ]
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'application/rss+xml, application/json' if mode == "rss" else 'application/json, text/html',
        'Cache-Control': 'no-cache'
    }

    try:
        # å®˜æ–¹æºå¿…é¡»é™é€Ÿ
        if 'reddit.com' in url: time.sleep(2)
        
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            return resp
        elif resp.status_code == 429:
            time.sleep(2)
    except:
        pass
    return None

# === 3. RSS å…œåº•æœºåˆ¶ (æ ¸å¿ƒæ–°å¢) ===
def fetch_via_rss(subreddit):
    """å½“ JSON å…¨æŒ‚æ—¶ï¼Œä½¿ç”¨ RSS è·å–åŸºç¡€æ•°æ®"""
    rss_url = f"https://www.reddit.com/r/{subreddit}/hot.rss"
    logger.info(f"   âš ï¸ JSON failed. Switching to RSS Fallback: {rss_url}")
    
    resp = make_request(rss_url, mode="rss")
    if not resp: 
        logger.error(f"   âŒ RSS also failed for r/{subreddit}")
        return []

    posts = []
    try:
        # è§£æ XML
        root = ET.fromstring(resp.content)
        # RSS å‘½åç©ºé—´é€šå¸¸æ˜¯ default, ä½†æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æå– entry
        # Atom æ ¼å¼é€šå¸¸ç”¨ {http://www.w3.org/2005/Atom}entry
        
        # ç®€å•æš´åŠ›è§£æï¼šéå†æ‰€æœ‰ entry æˆ– item
        # Reddit RSS æ ¼å¼æ˜¯ Atom
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        for entry in root.findall('atom:entry', ns):
            title = entry.find('atom:title', ns).text
            link = entry.find('atom:link', ns).attrib['href']
            # RSS æ²¡æœ‰ ID å­—æ®µï¼Œä» Link æˆªå–
            # Link ex: https://www.reddit.com/r/s/comments/1aje8x/title/
            try:
                post_id = link.split('/comments/')[1].split('/')[0]
            except:
                post_id = "rss_" + str(int(time.time())) + str(random.randint(1,1000))
                
            posts.append({
                "title": title,
                "id": post_id,
                "url": link,
                "score": 0,          # RSS æ— åˆ†æ•°
                "upvote_ratio": 1.0, # RSS æ— æƒ…ç»ªå€¼
                "num_comments": 0,   # RSS æ— è¯„è®ºæ•°
                "created_utc": time.time(),
                "subreddit": subreddit,
                "selftext": title,   # æ— è¯„è®ºï¼Œä»…ä¿ç•™æ ‡é¢˜
                "comments": []
            })
            if len(posts) >= 10: break
            
        logger.info(f"   âœ… RSS Salvation: Retrieved {len(posts)} posts (Titles only).")
    except Exception as e:
        logger.error(f"   âŒ RSS Parsing Error: {e}")
    
    return posts

# === 4. JSON æŠ“å–é€»è¾‘ ===
def fetch_json(path):
    for mirror in MIRRORS:
        url = f"{mirror}{path}"
        if '?' in url: url += f"&t={int(time.time())}"
        else: url += f"?t={int(time.time())}"
        
        resp = make_request(url, mode="json")
        if resp:
            try:
                data = resp.json()
                if isinstance(data, dict) and 'data' in data: return data
                if isinstance(data, list) and len(data) > 0: return data
            except: continue
    return None

def get_top_comments_text(post_id):
    """å°è¯•è·å–è¯„è®ºï¼Œæ‹¿ä¸åˆ°å°±è¿”å›ç©ºï¼Œä¸å¼ºæ±‚"""
    data = fetch_json(f"/comments/{post_id}.json")
    comments_list = []
    if data and isinstance(data, list) and len(data) > 1:
        try:
            children = data[1].get('data', {}).get('children', [])
            for child in children[:3]:
                body = child.get('data', {}).get('body')
                if body and body not in ['[deleted]', '[removed]']:
                    comments_list.append(body.replace('\n', ' ').strip())
        except: pass
    return " | ".join(comments_list)

# === 5. ä¸»å…¥å£ ===
def get_post_data(subreddit_name, post_limit=10, comment_limmit=5, reddit=None, posts_to_get="Hot"):
    logger.info(f"ğŸš€ [Fetch] r/{subreddit_name}...")
    
    # 1. å°è¯• JSON æ–¹å¼ (æ•°æ®æœ€å…¨)
    list_data = fetch_json(f"/r/{subreddit_name}/hot.json?limit={post_limit}")
    
    cleaned_posts = []
    
    # 2. å¦‚æœ JSON æˆåŠŸ
    if list_data and isinstance(list_data, dict) and 'data' in list_data:
        children = list_data['data'].get('children', [])
        for child in children:
            try:
                p = child['data']
                time.sleep(1) # ç¤¼è²Œä¼‘çœ 
                comments = get_top_comments_text(p['id'])
                cleaned_posts.append({
                    "title": p.get('title'),
                    "id": p.get('id'),
                    "url": f"https://www.reddit.com{p.get('permalink')}",
                    "score": p.get('score', 0),
                    "upvote_ratio": p.get('upvote_ratio', 1.0),
                    "num_comments": p.get('num_comments', 0),
                    "created_utc": p.get('created_utc'),
                    "subreddit": subreddit_name,
                    # ä¼˜å…ˆæ‹¼æ¥è¯„è®º
                    "selftext": f"{p.get('title')} . Top Comments: {comments}" if comments else p.get('title'),
                    "comments": []
                })
            except: continue
    
    # 3. å¦‚æœ JSON å¤±è´¥ (cleaned_posts ä¸ºç©º)ï¼Œè§¦å‘ RSS å…œåº•
    if not cleaned_posts:
        cleaned_posts = fetch_via_rss(subreddit_name)
        
    return cleaned_posts
